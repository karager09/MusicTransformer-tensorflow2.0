{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym przykładzie mamy doczynienia z generowaniem  ciągu (np. muzyki).  \n",
    "I tak jak podczas prezentacaji przedstawiłem model Transformera, tak tutaj będzie on wyglądał trochę inaczej.\n",
    "W tym przypadku bowiem nie interesuje nas model seq2seq (np. jak przy translacji),\n",
    "a model zdolny do generowania nowej muzyki.  \n",
    "Zatem z niczego (a w typ przypadku z kilku początkowych inputów, nut) mamy stworzyć pełną sekwencję.\n",
    "Dla przypomnienia klasyczny model Transformera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/publication/323904682/figure/fig1/AS:606458626465792@1521602412057/The-Transformer-model-architecture.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla generowania sekwencji nie jest potrzebna cała lewa strona, która jest Encoderem.\n",
    "W normalnym przypadku odpowiadałaby ona za \"wyciągnięcie\" pewnych informacji z ciągu wejściowego, lecz tutaj zwyczajnie go nie ma :)\n",
    "Zostaje więc strona prawa, czyli Decoder.\n",
    "Jest on odpowiedzialny za zakodowanie informacji, którą już zdobyliśmy (np. początek zdania w języku na który tłumaczymy) i wyciągnięcie z niej kolejnego wyrazu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak to działa?\n",
    "\n",
    "Na początku brany jest cały ciąg dotychczas wygenerowany i przepuszczany jest przez warstwę Positional Encoding, która dodaje informację na których miejscach znajdują się dane słowa (niektóre wagi połączeń są dzielone, więc trzeba dodatkową informację o pozycji).\n",
    "\n",
    "Następnie tak przygotowany ciąg przechodzi przez warstwę Masked Multi-Head Attention.  \n",
    "Dlaczego Masked?  \n",
    "Ponieważ dołożona jest \"maska\", która daje znać sieci, które miejsca w ciągu są wypełnione, a które są \"puste\" - czyli całkowicie do pominięcia przy obliczeniach.  \n",
    "Warto zauważyć, że do tej warstwy, wchodzą 3 wejścia i reprezentują one Key, Query i Value, a wszystkie one pochodzą z poprzedniej wartwy (dlatego jest to self-attention).\n",
    "\n",
    "Kolejna warstwa Add & Norm zbiera wyjście z Multi-Head Attention oraz oryginalny ciąg przed wejściem do tej warstwy. Wektory te odpowiednio dodaje i normalizuje.\n",
    "\n",
    "W klasycznym Transformerze teraz następowałoby połączenia danych z Encodera (wtedy Key i Values w mechanizmie attention pochodziłyby z oryginalnego ciągu. Przy generowaniu muzyki nie ma takiej potrzeby, więc znowu może to być self-attention.\n",
    "\n",
    "Znowu występuje warstwa Add & Norm, po czym jest warstwa feedforward i kolejna Add & Norm.\n",
    "\n",
    "Co ważne taka konfiguracja jest powtarzana kilka razy!\n",
    "\n",
    "Na samym końcu znajduje się warstwa liniowa i softmax w celu zebrania wyniku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chciałbym się tutaj skupić właśnie na mechanizmie self-attention, ponieważ reszta jest pewnie znajoma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla podsumowania.\n",
    "Mamy ciąg np. słów, zapisanych w postaci wektorów.  \n",
    "Po zastosowaniu na nim mechanizmu attention otrzymujemy nową macierz, która jest reprezentacją tamtego ciągu.  \n",
    "W transformerze operację taką wykonujemy wiele razy, na końcu decydując jakie słowo powinno wystąpić kolejne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak wygląda obliczanie attention (na podstawie  \n",
    "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a  \n",
    "https://medium.com/lsc-psd/introduction-of-self-attention-layer-in-transformer-fc7bff63f3bc):  \n",
    "Założenie:  \n",
    "Wejście: input_1, input_2, input_3 to kolejne słowa w zdaniu.  \n",
    "  \n",
    "1. Przygotowanie inputów.\n",
    "2. Inicjalizacja wag macierzy Wk, Wq, Wv (przygotowanie).\n",
    "3. Obliczenie key, query i value.\n",
    "4. Obliczenie wartości attention dla inputu_1.\n",
    "5. Obliczenie funkcji softmax.\n",
    "6. Pomnożenie tych wartości z values.\n",
    "7. Zsumowanie tych wartości i otrzymanie Outputu 1.\n",
    "8. Powtórzenie kroków 4–7 dla Inputu 2 i 3 (kroki te można wykonać równolegle poprzez odpowiednie zarządzanie macierzami)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0],\n",
       "       [0, 2, 0, 2],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Krok 1: te \"inputy\" to kolejne np słowa w ciągu, zakodowane jako wektory\n",
    "import numpy as np\n",
    "input_1 = np.array([1, 0, 1, 0])\n",
    "input_2 = np.array([0, 2, 0, 2])\n",
    "input_3 = np.array([1, 1, 1, 1])\n",
    "\n",
    "input = np.array([input_1, input_2, input_3])  # w postaci macierzowej\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 1, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Krok 2\n",
    "# Każdy input ma swoje wartości odpowiadających mu wektorów w postaci key, query, value.\n",
    "# Aby otrzymać te wartości input przepuszczany jest przez warstwę w pełni połączonych neuronów \n",
    "# (czyli generalnie wykonywane jest mnożenie przez macierz - każda z nich (Wk, Wq, Wv ma swoje wagi)).\n",
    "# Wagi te są na początku inicjalizowane małymi liczbmi, a następnie sieć się ich uczy. \n",
    "# W tym przykładzie rozmiar key, query i value będzie wynosił 3 (rozmiar inputu to 4).\n",
    "\n",
    "Wk = np.array(  [[0, 0, 1],\n",
    "                 [1, 1, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 1, 0]])\n",
    "\n",
    "Wq = np.array(  [[1, 0, 1],\n",
    "                 [1, 0, 0],\n",
    "                 [0, 0, 1],\n",
    "                 [0, 1, 1]])\n",
    "\n",
    "Wv = np.array(  [[0, 2, 0],\n",
    "                 [0, 3, 0],\n",
    "                 [1, 0, 3],\n",
    "                 [1, 1, 0]])\n",
    "Wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Krok 3\n",
    "# Obliczenie wartości key, query i value dla inputu_1.\n",
    "\n",
    "#                [0, 0, 1]\n",
    "# [1, 0, 1, 0] x [1, 1, 0] = [0, 1, 1]\n",
    "#                [0, 1, 0]\n",
    "#                [1, 1, 0]\n",
    "key_1 = input_1.dot(Wk)\n",
    "key_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key for input 1: \n",
      "[0 1 1]\n",
      "\n",
      "Key: \n",
      "[[0 1 1]\n",
      " [4 4 0]\n",
      " [2 3 1]]\n",
      "Query: \n",
      "[[1 0 2]\n",
      " [2 2 2]\n",
      " [2 1 3]]\n",
      "Value: \n",
      "[[1 2 3]\n",
      " [2 8 0]\n",
      " [2 6 3]]\n"
     ]
    }
   ],
   "source": [
    "# Obliczanie dla wszystkich inputów\n",
    "key_1 = input_1.dot(Wk)\n",
    "query_1 = input_1.dot(Wq)\n",
    "value_1 = input_1.dot(Wv)\n",
    "\n",
    "# ...\n",
    "# w skrócie w postaci macierzowej\n",
    "key_matrix = input.dot(Wk)\n",
    "query_matrix = input.dot(Wq)\n",
    "value_matrix = input.dot(Wv)\n",
    "\n",
    "print(f'Key for input 1: \\n{key_1}\\n')\n",
    "print(f'Key: \\n{key_matrix}')\n",
    "print(f'Query: \\n{query_matrix}')\n",
    "print(f'Value: \\n{value_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*Cfsh9uK8Y6FhamziJZIKRA.jpeg\" width=600 height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 7, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Krok 4: Wyliczenie wartości attention dla input_1\n",
    "# Sprawdzamy jak blisko query zgadza się z każdym key. Najprostszym sposobem jest iloczyn skalarny (dot product attention):\n",
    "# (Na obrazku mamy dodatkowe skalowanie przez pierwiastek z wymiaru. Nie zgadzają się też wymiary.)\n",
    "attention_score_1 = query_1.dot(key_matrix)\n",
    "attention_score_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*4Ky7WD2Bwt7ONuewCEimbg.gif\" width=600 height=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04661262, 0.93623955, 0.01714783])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Krok 5: Skalowanie za pomocą funkcji softmax\n",
    "from scipy.special import softmax\n",
    "soft = softmax(attention_score_1)\n",
    "soft\n",
    "# Widać że input_1 jest mocno zależny od input_2, ponieważ warość attention_score jest dużo większa niż dla reszty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ten sposób obliczone została zgodność/zależność input_1 od reszty ciągu.\n",
    "Jest to również waga z jaką odpowiadające im wartości wejdą do rezultatu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04661262 0.09322525 0.13983787]\n",
      "[1.8724791  7.48991642 0.        ]\n",
      "[0.03429565 0.10288695 0.05144348]\n"
     ]
    }
   ],
   "source": [
    "# Krok 6: Pomnożenie otrzymanych wag z values\n",
    "inp_1_val_1 = soft[0] * value_1\n",
    "inp_1_val_2 = soft[1] * value_2\n",
    "inp_1_val_3 = soft[2] * value_3\n",
    "print(inp_1_val_1)\n",
    "print(inp_1_val_2)\n",
    "print(inp_1_val_3)\n",
    "# w ten sposób otrzymaliśmy tablicę, która zawiera zsumowane inputy przeskalowane względem zgodności (softmax)\n",
    "# poniższy wynik otrzymany został dla input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.95338738, 7.68602861, 0.19128134])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Krok 7: Zsumowanie tych wartości do jednego wektora\n",
    "result = inp_1_val_1 + inp_1_val_2 + inp_1_val_3\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*tIU60poFU4Ym988ULlN1sA.gif\" width=600 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krok 8\n",
    "Powtórzenie obliczeń 4-7 dla inputu 2 i 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ten sposób otrzymaliśmy nowy wektor dla input_1, który wziął pod uwagę inne inputy i stworzył całkiem nową reprezentacje ciągu. Podobne działania wykonujemy dla reszty inputów, w rezultacie otrzymując nową sekwencję.  \n",
    "Jak zaznaczyłem przepuszczamy ją przez normalizację i uwaga:  \n",
    "całość powtarzamy jeszcze kilka razy :D  \n",
    "W ten sposób uzyskujemy wewnętrzną strukturę ciągu i reprezentację wiadomości jaka się za nim kryje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*F2dNXYpvLwbqGLZtK0rFFQ.jpeg\" width=600 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ale to jeszcze nie koniec :o  \n",
    "Przedstawiłem tutaj zwykły mechanizm self-attention (bez self byłoby gdyby key i values, pochodziły z innego ciągu). Ale na rysunku Transformera widnieje \"Multi-Head\". Dlaczego?  \n",
    "Aby poprawić działanie całego procesu, wejściową sekwencje przepuszczamy przez wiele różnych wag Wk, Wq, Wv za każdym razie otrzymując trochę co innego.  \n",
    "Dopiero później je łączymy w jedną reprezentację i otrzymujemy wyjście z Multi-Head.  \n",
    "A później normalizacja itp. i zaczynamy zabawę od początku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*65w5woXDym6xClP8tqOifg.gif\" width=600 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dlatego właśnie wyniki otrzymywane tą metodą są bardzo dobre - za pomocą wielu wag, możemy zwrócić szczególną uwagę na konkrenty zestaw wiadomości i później połączyć je w jedną."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
